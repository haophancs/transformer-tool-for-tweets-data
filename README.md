# CT-BERT Fine tunning
Cite as 
**@inproceedings{tran-etal-2020-uit,
    title = "{UIT}-{HSE} at {WNUT}-2020 Task 2: Exploiting {CT}-{BERT} for Identifying {COVID}-19 Information on the {T}witter Social Network",
    author = "Tran, Khiem  and
      Phan, Hao  and
      Nguyen, Kiet  and
      Thuy Nguyen, Ngan Luu",
    booktitle = "Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.wnut-1.53",
    pages = "383--387",
    abstract = "Recently, COVID-19 has affected a variety of real-life aspects of the world and led to dreadful consequences. More and more tweets about COVID-19 has been shared publicly on Twitter. However, the plurality of those Tweets are uninformative, which is challenging to build automatic systems to detect the informative ones for useful AI applications. In this paper, we present our results at the W-NUT 2020 Shared Task 2: Identification of Informative COVID-19 English Tweets. In particular, we propose our simple but effective approach using the transformer-based models based on COVID-Twitter-BERT (CT-BERT) with different fine-tuning techniques. As a result, we achieve the F1-Score of 90.94{\%} with the third place on the leaderboard of this task which attracted 56 submitted teams in total.",
}
